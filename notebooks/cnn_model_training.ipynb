{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9c48a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 14:08:34.035093: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/div/vsCode/Audio_classifier_Model/data/urbansound8k_spectrograms'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGPU Available:\u001b[39m\u001b[33m\"\u001b[39m, tf.config.list_physical_devices(\u001b[33m'\u001b[39m\u001b[33mGPU\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m     11\u001b[39m SPECTRO_ROOT = Path(\u001b[33m\"\u001b[39m\u001b[33m/home/div/vsCode/Audio_classifier_Model/data/urbansound8k_spectrograms\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m CLASS_NAMES = \u001b[38;5;28msorted\u001b[39m(\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mSPECTRO_ROOT\u001b[49m\u001b[43m.\u001b[49m\u001b[43miterdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     13\u001b[39m CLASS_TO_INDEX = {name: idx \u001b[38;5;28;01mfor\u001b[39;00m idx, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(CLASS_NAMES)}\n\u001b[32m     14\u001b[39m MAX_FRAMES = \u001b[32m174\u001b[39m  \u001b[38;5;66;03m# Adjust based on spectrogram dimensions\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/pathlib.py:1058\u001b[39m, in \u001b[36mPath.iterdir\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34miterdir\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m   1053\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Yield path objects of the directory contents.\u001b[39;00m\n\u001b[32m   1054\u001b[39m \n\u001b[32m   1055\u001b[39m \u001b[33;03m    The children are yielded in arbitrary order, and the\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    special entries '.' and '..' are not included.\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1058\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[32m   1059\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_child_relpath(name)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: '/home/div/vsCode/Audio_classifier_Model/data/urbansound8k_spectrograms'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from pathlib import Path\n",
    "from typing import Tuple\n",
    "import numpy as np\n",
    "from random import sample\n",
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "SPECTRO_ROOT = Path(\"/home/div/vsCode/Audio_classifier_Model/data/urbansound8k_spectrograms\")\n",
    "CLASS_NAMES = sorted([d.name for d in SPECTRO_ROOT.iterdir() if d.is_dir()])\n",
    "CLASS_TO_INDEX = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "MAX_FRAMES = 174  # Adjust based on spectrogram dimensions\n",
    "tf_files = []\n",
    "tf_labels = []\n",
    "\n",
    "# Enable memory growth to prevent full GPU allocation\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Memory growth enabled for {len(gpus)} GPU(s)\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa97ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 2732\n",
      "  Class 'car_horn': 429 samples\n",
      "  Class 'dog_bark': 1000 samples\n",
      "  Class 'gun_shot': 374 samples\n",
      "  Class 'siren': 929 samples\n"
     ]
    }
   ],
   "source": [
    "# Setting up file paths and labels\n",
    "\n",
    "for cls in CLASS_NAMES:\n",
    "    for npy in (SPECTRO_ROOT / cls).glob(\"*.npy\"):\n",
    "        tf_files.append(str(npy))\n",
    "        tf_labels.append(CLASS_TO_INDEX[cls])\n",
    "tf_files = np.array(tf_files)\n",
    "tf_labels = np.array(tf_labels, dtype=np.int32)\n",
    "\n",
    "def load_npy(path, label):\n",
    "    spec = np.load(path.decode(\"utf-8\")).astype(np.float32)\n",
    "    spec = (spec - spec.mean()) / (spec.std() + 1e-6)\n",
    "\n",
    "    if spec.shape[1] < MAX_FRAMES:\n",
    "        pad = MAX_FRAMES - spec.shape[1]\n",
    "        spec = np.pad(spec, ((0, 0), (0, pad)), mode=\"constant\")\n",
    "    else:\n",
    "        spec = spec[:, :MAX_FRAMES]\n",
    "\n",
    "    spec = np.expand_dims(spec, axis=-1)\n",
    "    return spec, label\n",
    "\n",
    "def tf_loader(paths, labels, batch_size=32, shuffle=True):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(paths))\n",
    "    ds = ds.map(\n",
    "        lambda p, l: tf.numpy_function(load_npy, [p, l], [tf.float32, tf.int32]),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "    ds = ds.map(\n",
    "        lambda x, y: (tf.ensure_shape(x, [1025, MAX_FRAMES, 1]), tf.ensure_shape(y, [])),\n",
    "        num_parallel_calls=tf.data.AUTOTUNE,\n",
    "    )\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "# Dataset printout\n",
    "print(f\"Total samples: {len(tf_files)}\")\n",
    "for cls in CLASS_NAMES:\n",
    "    count = np.sum(tf_labels == CLASS_TO_INDEX[cls])\n",
    "    print(f\"  Class '{cls}': {count} samples\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcfe2ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 169ms/step - accuracy: 0.6683 - loss: 0.8168 - val_accuracy: 0.1264 - val_loss: 4.7679\n",
      "Epoch 2/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 169ms/step - accuracy: 0.6683 - loss: 0.8168 - val_accuracy: 0.1264 - val_loss: 4.7679\n",
      "Epoch 2/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.6972 - loss: 0.6602 - val_accuracy: 0.1502 - val_loss: 5.5280\n",
      "Epoch 3/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.6972 - loss: 0.6602 - val_accuracy: 0.1502 - val_loss: 5.5280\n",
      "Epoch 3/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 142ms/step - accuracy: 0.7466 - loss: 0.5931 - val_accuracy: 0.1392 - val_loss: 5.9816\n",
      "Epoch 4/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 142ms/step - accuracy: 0.7466 - loss: 0.5931 - val_accuracy: 0.1392 - val_loss: 5.9816\n",
      "Epoch 4/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 138ms/step - accuracy: 0.7754 - loss: 0.5228 - val_accuracy: 0.1630 - val_loss: 6.6742\n",
      "Epoch 5/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 138ms/step - accuracy: 0.7754 - loss: 0.5228 - val_accuracy: 0.1630 - val_loss: 6.6742\n",
      "Epoch 5/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.8216 - loss: 0.4593 - val_accuracy: 0.1648 - val_loss: 6.7870\n",
      "Epoch 6/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.8216 - loss: 0.4593 - val_accuracy: 0.1648 - val_loss: 6.7870\n",
      "Epoch 6/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 142ms/step - accuracy: 0.8312 - loss: 0.4212 - val_accuracy: 0.1392 - val_loss: 7.9334\n",
      "Epoch 7/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 142ms/step - accuracy: 0.8312 - loss: 0.4212 - val_accuracy: 0.1392 - val_loss: 7.9334\n",
      "Epoch 7/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.8408 - loss: 0.4111 - val_accuracy: 0.1447 - val_loss: 8.2561\n",
      "Epoch 8/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.8408 - loss: 0.4111 - val_accuracy: 0.1447 - val_loss: 8.2561\n",
      "Epoch 8/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.8435 - loss: 0.3926 - val_accuracy: 0.1685 - val_loss: 7.7113\n",
      "Epoch 9/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.8435 - loss: 0.3926 - val_accuracy: 0.1685 - val_loss: 7.7113\n",
      "Epoch 9/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 138ms/step - accuracy: 0.8472 - loss: 0.3884 - val_accuracy: 0.1484 - val_loss: 8.0173\n",
      "Epoch 10/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 138ms/step - accuracy: 0.8472 - loss: 0.3884 - val_accuracy: 0.1484 - val_loss: 8.0173\n",
      "Epoch 10/10\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.8586 - loss: 0.3592 - val_accuracy: 0.1685 - val_loss: 7.8992\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 141ms/step - accuracy: 0.8586 - loss: 0.3592 - val_accuracy: 0.1685 - val_loss: 7.8992\n",
      "\n",
      "Final training metrics:\n",
      "  accuracy: 0.8586\n",
      "  loss: 0.3592\n",
      "  val_accuracy: 0.1685\n",
      "  val_loss: 7.8992\n",
      "\u001b[1m 1/69\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 82ms/step - accuracy: 0.0000e+00 - loss: 8.7555\n",
      "Final training metrics:\n",
      "  accuracy: 0.8586\n",
      "  loss: 0.3592\n",
      "  val_accuracy: 0.1685\n",
      "  val_loss: 7.8992\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.1685 - loss: 7.8992\n",
      "\n",
      "Validation evaluation:\n",
      "  accuracy: 0.1685\n",
      "  loss: 7.8992\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 31ms/step - accuracy: 0.1685 - loss: 7.8992\n",
      "\n",
      "Validation evaluation:\n",
      "  accuracy: 0.1685\n",
      "  loss: 7.8992\n"
     ]
    }
   ],
   "source": [
    "# Creating training and validation datasets\n",
    "\n",
    "split = int(len(tf_files) * 0.2)\n",
    "train_ds = tf_loader(tf_files[split:], tf_labels[split:], batch_size=8, shuffle=True)\n",
    "val_ds = tf_loader(tf_files[:split], tf_labels[:split], batch_size=8, shuffle=False)\n",
    "\n",
    "# Building and training the CNN model\n",
    "tf_model = tf.keras.Sequential([\n",
    "    # tf.keras.layers.Input(shape=(1025, MAX_FRAMES, 1)),\n",
    "    tf.keras.layers.Conv2D(64, 1, padding=\"same\", activation=\"relu\", input_shape=(1025, MAX_FRAMES, 1)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(128, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Conv2D(256, 3, padding=\"same\", activation=\"relu\"),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(len(CLASS_NAMES), activation=\"softmax\"),\n",
    "])\n",
    "\n",
    "tf_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-3),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[\"accuracy\"],\n",
    ")\n",
    "\n",
    "# Train and report metrics\n",
    "history = tf_model.fit(train_ds, validation_data=val_ds, epochs=10)\n",
    "\n",
    "train_metrics = {metric: values[-1] for metric, values in history.history.items()}\n",
    "print(\"\\nFinal training metrics:\")\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")\n",
    "\n",
    "eval_results = tf_model.evaluate(val_ds, return_dict=True)\n",
    "print(\"\\nValidation evaluation:\")\n",
    "for metric, value in eval_results.items():\n",
    "    print(f\"  {metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43d6784",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
